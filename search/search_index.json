{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/06/01/fifo-is-better-than-lru-the-power-of-lazy-promotion-and-quick-demotion/","title":"FIFO is Better than LRU: the Power of Lazy Promotion and Quick Demotion","text":"<p>TL;DR </p> <p>Historically FIFO-based algorithms are thought to be less efficient (having higher miss ratios) than LRU-based algorithms. In this blog, we introduce two techniques, lazy promotion, which promotes objects only at eviction time, and quick demotion, which removes most new objects quickly. We will show that</p> <ul> <li>Conventional-wisdom-suggested \"weak LRUs\", e.g., FIFO-Reinsertion, is actually more efficient (having lower miss ratios) than LRU;</li> <li>Simply evicting most new objects can improve state-of-the-art algorithm's efficiency.</li> <li>Eviction algorithms can be designed like building LEGOs by adding lazy promotion and quick demotion on top of FIFO.</li> </ul>"},{"location":"blog/2023/06/01/fifo-is-better-than-lru-the-power-of-lazy-promotion-and-quick-demotion/#background","title":"Background","text":"<p>Caching is a well-known and widely deployed technique to speed up data access, reduce repeated computation and data transfer.  A core component of a cache is the eviction algorithm, which chooses the objects stored in the limited cache space. Two metrics describe the performance of an eviction algorithm: efficiency measured by the miss ratio and throughput measured by the number of requests served per second.</p> <p>The study of cache eviction algorithms has a long history, with a majority of the work centered around LRU (that is, to evict the least-recently-used object). LRU maintains a doubly-linked list, promoting objects to the head of the list upon cache hits and evicting the object at the tail of the list when needed. Belady and others found that memory access patterns often exhibit temporal locality --- \u201cthe most recently used pages were most likely to be reused in the immediate future\u201d. Thus, LRU using recency to promote objects was found to be better than FIFO.</p> <p>Most eviction algorithms designed to achieve high efficiency start from LRU. For example, many algorithms, such as ARC, SLRU, 2Q, MQ, and multi-generational LRU, use multiple LRU queues to separate hot and cold objects. Some algorithms, e.g., LIRS and LIRS2, maintain an LRU queue but use different metrics to promote objects. While other algorithms, e.g., LRFU, EE-LRU, LeCaR, and CACHEUS, augment LRU's recency with different metrics. In addition, many recent works, e.g., Talus, improve LRU's ability to handle scan and loop requests.</p> <p>Besides efficiency, there have been fruitful studies on enhancing the cache's throughput performance and thread scalability. Each cache hit in LRU promotes an object to the head of the queue, which requires updating at least six pointers guarded by locks. These overheads are not acceptable in many deployments that need high performance. Thus, performance-centric systems often use FIFO-based algorithms to avoid LRU's overheads. For example, FIFO-Reinsertion and variants of CLOCK have been developed, which serve as LRU approximations. It is often perceived that these algorithms trade miss ratio for better throughput and scalability.</p> <p>In this blog, I am going to show that FIFO is in-fact better than LRU not only because of higher throughput, more scalable, but also more efficient and effective (having lower miss ratios).</p>"},{"location":"blog/2023/06/01/fifo-is-better-than-lru-the-power-of-lazy-promotion-and-quick-demotion/#why-fifo-and-what-it-needs","title":"Why FIFO and What it needs","text":"<p>FIFO has many benefits over LRU.  For example, FIFO has less metadata and requires no metadata update on each cache hit, and thus is faster and more scalable than LRU. In contrast, LRU requires updating six pointers on each cache hit, which is not friendly for modern computer architecture due to random memory accesses. Moreover, FIFO is always the first choice when implementing a flash cache because it does not incur write amplification. Although FIFO has throughput and scalability benefits, it is common wisdom that FIFO is less effective (having higher miss ratio) than LRU.</p> <p> A cache can be viewed as a logically ordered queue with four operations: insertion, removal, promotion and demotion. Most eviction algorithms are promotion algorithms.  <p></p> <p>To understand the various factors that affect the miss ratio, we introduce a cache abstraction.  A cache can be viewed as a logically total-ordered queue with four operations: insertion, removal, promotion, and demotion. Objects in the cache can be compared and ordered based on some metric (e.g., time since the last request), and the eviction algorithm evicts the least valuable object based on the metric. Insertion and removal are user-controlled operations, where removal can either be directly invoked by the user or indirectly via the use of time-to-live (TTL). Promotion and demotion are internal operations of the cache used to maintain the logical ordering between objects.</p> <p>We observe that most eviction algorithms use promotion to update the ordering between objects. For example, all the LRU-based algorithms promote objects to the head of the queue on cache hits, which we call eager promotion. Meanwhile, demotion is performed implicitly: when an object is promoted, other objects are passively demoted. We call this process passive demotion, a slow process as objects need to traverse through the cache queue before being evicted. However, we will show that instead of eager promotion and passive demotion, eviction algorithms should use lazy promotion and quick demotion.</p>"},{"location":"blog/2023/06/01/fifo-is-better-than-lru-the-power-of-lazy-promotion-and-quick-demotion/#lazy-promotion","title":"Lazy Promotion","text":"<p>To avoid popular objects from being evicted while not incurring much performance overhead, we propose adding lazy promotion on top of FIFO (called  LP-FIFO), which promotes objects only when they are about to be evicted. lazy promotion aims to retain popular objects with minimal effort. An example is FIFO-Reinsertion (note that FIFO-Reinsertion, 1-bit CLOCK, and Second Chance are different implementations of the same eviction algorithm): an object is reinserted at eviction time if it has been requested while in the cache. </p> <p>LP-FIFO has several benefits over eager promotion (promoting on every access) used in LRU-based algorithms. First, LP-FIFO inherits FIFO's throughput and scalability benefits because few metadata operations are needed when an object is requested. For example, FIFO-Reinsertion only needs to update a Boolean field upon the first request to a cached object without locking. Second, performing promotion at eviction time allows the cache to make better decisions by accumulating more information about the objects, e.g., how many times an object has been requested.</p> Trace approx time #trace cache type #req (millions) #obj (millions) MSR 2007 13 block 410 74 FIU 2008 9 block 514 20 Cloudphysics 2015 106 block 2,114 492 Major CDN 2018 219 object 3,728 298 Tencent Photo 2018 2 object 5,650 1,038 Wiki CDN 2019 3 object 2,863 56 Tencent CBS 2020 4030 block 33,690 551 Alibaba 2020 652 block 19,676 1702 Twitter 2020 54 KV 195,441 10,650 Social Network 2020 219 KV 549,784 42,898 <p>To understand LP-FIFO's efficiency, we performed a large-scale simulation study on 5307 production traces from 10 data sources, which include open-source and proprietary datasets collected between 2007 and 2020. The 10 datasets contain 814 billion (6,386 TB) requests and 55.2 billion (533 TB) objects, and cover different types of caches, including block, key-value (KV), and object caches. We further divide the traces into block and web (including Memcached and CDN). We choose small/large cache size as 0.1%/10% of the number of unique objects in the trace.</p> <p>We compare the miss ratios of LRU with two LP-FIFO algorithms: FIFO-Reinsertion and 2-bit CLOCK. 2-bit CLOCK tracks object frequency up to three, and an object's frequency decreases by one each time the CLOCK hand scans through it. Objects with frequency zero are evicted.</p> <p>Common wisdom suggests that these two LP-FIFO examples are LRU approximations and will exhibit higher miss ratios than LRU. However, we found that LP-FIFO often exhibits miss ratios lower than LRU.</p> <p> Comparison of FIFO-Reinsertion, 2-bit CLOCK and LRU on 10 datasets with 5307 traces. Left two: small cache, right two: large cache. A longer bar means the algorithm is more efficient (having lower miss ratios on more traces). Note that we do not consider the overhead of LRU metadata in all the evaluations.  <p></p> <p>The figure above shows that FIFO-Reinsertion and 2-bit CLOCK are better than LRU on most traces. Specifically, FIFO-Reinsertion is better than LRU on 9 and 7 of the 10 datasets using a small and large cache size, respectively. Moreover, on half of the datasets, more than 80% of the traces in each dataset favor FIFO-Reinsertion over LRU at both sizes. On the two social network datasets, LRU is better than FIFO-Reinsertion (especially at the large cache size). This is because most objects in these two datasets are accessed more than once, and using one bit to track object access is insufficient. Therefore, when increasing the one bit in FIFO-Reinsertion (CLOCK) to two bits (2-bit CLOCK), we observe that the number of traces favoring LP-FIFO increases to around 70%. Across all datasets, 2-bit CLOCK is better than FIFO on all datasets at the small cache size and 9 of the 10 datasets at the large cache size.</p> <p> FIFO-Reinsertion demotes new objects faster than LRU because objects requested before the new object also pushes it down the queue.  <p></p> <p>Two reasons contribute to LP-FIFO's high efficiency. First, lazy promotion often leads to quick demotion. For example, under LRU, a newly-inserted object G is pushed down the queue only by (1) new objects and (2) cached objects that are requested after G. However, besides the objects requested after G, the objects requested before G (but have not been promoted, e.g., B, D) also push G down the queue when using FIFO-Reinsertion. Second, compared to promotion at each request, object ordering in LP-FIFO is closer to the insertion order, which we conjecture is better suited for many workloads that exhibit popularity decay --- old objects have a lower probability of getting a request.</p> <p>While LP-FIFO surprisingly wins over LRU in miss ratio, it cannot outperform state-of-the-art algorithms. We next discuss another building block that bridges this gap.</p>"},{"location":"blog/2023/06/01/fifo-is-better-than-lru-the-power-of-lazy-promotion-and-quick-demotion/#quick-demotion","title":"Quick Demotion","text":"<p>Efficient eviction algorithms not only need to keep popular objects in the cache but also need to evict unpopular objects fast. In this section, we show that quick demotion (QD) is critical for an efficient eviction algorithm, and it enables FIFO-based algorithms to achieve state-of-the-art efficiency.</p> <p>Because demotion happens passively in most eviction algorithms, an object typically traverses through the cache before being evicted. Such traversal gives each object a good chance to prove its value to be kept in the cache. However, cache workloads often follow Zipf popularity distribution, with most objects being unpopular. This is further exacerbated by (1) the scan and loop access patterns in the block cache workloads, and (2) the vast existence of dynamic and short-lived data, the use of versioning in object names, and the use of short TTLs in the web cache workloads. We believe the opportunity cost of new objects demonstrating their values is often too high: the object being evicted at the tail of the queue may be more valuable than the objects recently inserted.</p> <p> An example of quick demotion: adding a small FIFO to filter most new objects that do not have a request soon after insertion.  <p></p> <p>To illustrate the importance of quick demotion, we add a simple QD technique on top of state-of-the-art eviction algorithms. The QD technique consists of a small probationary FIFO queue storing cached data and a ghost FIFO queue storing metadata of objects evicted from the probationary FIFO queue. The probationary FIFO queue uses 10% of the cache space and acts as a filter for unpopular objects: objects not requested after insertion are evicted early from the FIFO queue. The main cache runs a state-of-the-art algorithm and uses 90% of the space. And the ghost FIFO stores as many entries as the main cache. Upon a cache miss, the object is written into the probationary FIFO queue unless it is in the ghost FIFO queue, in which case, it is written into the main cache. When the probationary FIFO queue is full, if the object to evict has been accessed since insertion, it is inserted into the main cache. Otherwise, it is evicted and recorded in the ghost FIFO queue.</p> <p>We add this FIFO-based QD technique to five state-of-the-art eviction algorithms, ARC, LIRS, CACHEUS, LeCaR, and LHD. We used the open-source LHD implementation from the authors, implemented the others following the corresponding papers, and cross-checked with open-source implementations. We evaluated the QD-enhanced and original algorithms on the 5307 traces. Because the traces have a wide range of miss ratios, we choose to present each algorithm's miss ratio reduction from FIFO calculated as (mr<sub>FIFO</sub> - mr<sub>algo</sub>) / mr<sub>FIFO</sub>.</p> <p> On the block traces, quick demotion can improve most state-of-the-art algorithm's efficiency. Left: small cache, right: large cache.  <p></p> <p> On the web traces, quick demotion can improve all state-of-the-art algorithm's efficiency. Left: small cache, right: large cache.  <p></p> <p>The figures above show that the QD-enhanced algorithms further reduce the miss ratio of each state-of-the-art algorithm on almost all percentiles. For example, QD-ARC (QD-enhanced ARC) reduces ARC's miss ratio by up to 59.8% with a mean reduction of 1.5% across all workloads on the two cache sizes, QD-LIRS reduces LIRS's miss ratio by up to 49.6% with a mean of 2.2%, and QD-LeCaR reduces LeCaR's miss ratio by up to 58.8% with a mean of 4.5%. Note that achieving a large miss ratio reduction on a large number of diverse traces is non-trivial. For example, the best state-of-the-art algorithm, ARC, can only reduce the miss ratio of LRU 6.2% on average.</p> <p>The gap between the QD-enhanced algorithm and the original algorithm is wider (1) when the state-of-the-art is relatively weak, (2) when the cache size is large, and (3) on the web workloads. With a weaker state-of-the-art, the opportunity for improvement is larger, allowing QD to provide more prominent benefits. For example, QD-LeCaR reduces LeCaR's miss ratios by 4.5% on average, larger than the reductions on other state-of-the-art algorithms. When the cache size is large, unpopular objects spend more time in the cache, and quick demotion becomes more valuable. For example, QD-ARC and ARC have similar miss ratios on the block workloads at the small cache size. But QD-ARC reduces ARC's miss ratio by 2.3% on average at the large cache size. However, when the cache size is too large, e.g., 80% of the number of objects in the trace, adding QD may increase the miss ratio (not shown). At last, QD provides more benefits on the web workloads than the block workloads, especially when the cache size is small. We conjecture that web workloads have more short-lived data and exhibit stronger popularity decay, which leads to a more urgent need for quick demotion. While quick demotion improves the efficiency of most state-of-the-art algorithms, for a small subset of traces, QD may increase the miss ratio when the cache size is small because the probationary FIFO is too small to capture some potentially popular objects.</p> <p>Although adding the probationary FIFO improves efficiency, it further increases the complexity of the already complicated state-of-the-art algorithms. To reduce complexity, we add the same QD technique on top of 2-bit CLOCK and call it QD-LP-FIFO. QD-LP-FIFO uses two FIFO queues to cache data and a ghost FIFO queue to track evicted objects. It is not hard to see QD-LP-FIFO is simpler than all state-of-the-art algorithms --- it requires at most one metadata update on a cache hit and no locking for any cache operation. Therefore, we believe it will be faster and more scalable than all state-of-the-art algorithms. Besides enjoying all the benefits of simplicity, QD-LP-FIFO also achieves lower miss ratios than state-of-the-art algorithms. For example, compared to LIRS and LeCaR, QD-LP-FIFO reduces miss ratio by 1.6% and 4.3% on average, respectively, across the 5307 traces. While the goal of this work is not to propose a new eviction algorithm, QD-LP-FIFO illustrates how we can build simple yet efficient eviction algorithms by adding quick demotion and lazy promotion techniques to a simple base eviction algorithm such as FIFO.</p>"},{"location":"blog/2023/06/01/fifo-is-better-than-lru-the-power-of-lazy-promotion-and-quick-demotion/#discussion","title":"Discussion","text":"<p>We have demonstrated reinsertion as an example of LP and the use of a small probationary FIFO queue as an example of QD. However, these are not the only techniques. For example, reinsertion can leverage different metrics to decide whether the object should be reinserted. Besides reinsertion, several other techniques are often used to reduce promotion and improve scalability, e.g., periodic promotion, batched promotion, promoting old objects only, and promoting with try-lock.  Although these techniques do not fall into our strict definition of lazy promotion (promotion on eviction), many of them effectively retain popular objects from being evicted. On the quick demotion side, besides the small probationary FIFO queue, one can leverage other techniques to define and discover unpopular objects such as Hyperbolic and LHD. Moreover, admission algorithms, e.g., TinyLFU, Bloom Filter, probabilistic, and ML-based admission algorithms, can be viewed as a form of QD --- albeit some of them are too aggressive at demotion (rejecting objects from entering the cache).</p> <p>Note that QD bears similarity with some generational garbage collection algorithms, which separately store short-lived and long-lived data in young-gen and old-gen heaps. Therefore, ideas from garbage collection may be borrowed to strengthen cache eviction algorithms.</p> <p>The design of QD-LP-FIFO opens the door to designing simple yet efficient cache eviction algorithms by innovating on LP and QD techniques. And we envision future eviction algorithms can be designed like building LEGO --- adding lazy promotion and quick demotion on top of a base eviction algorithm.</p>"},{"location":"blog/2023/06/01/fifo-is-better-than-lru-the-power-of-lazy-promotion-and-quick-demotion/#acknowledgement","title":"Acknowledgement","text":"<p>There are many people I would like to thank, including but not limited to my co-authors, Carnegie Mellon University Parallel Data Lab (and our sponsors), and Cloudlab.  I would also like to give a big shoutout to the people and organizations that open-sourced the traces, without which, this work is not possible and we will NEVER know that CLOCK is better than LRU on every aspect! </p> <p>This work is published at HotOS'23, more details can be found here, and the slides can be found here. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/","title":"FIFO queues are all you need for cache eviction","text":"<p>TL;DR </p> <p>In this blog, I will describe a simple, scalable FIFO-based eviction algorithm with three static queues (S3-FIFO). Evaluated on 6594 cache traces from 14 datasets, we show that S3-FIFO has lower miss ratios than 12 state-of-the-art algorithms. Moreover, S3-FIFO\u2019s efficiency is robust \u2014 it has the lowest mean miss ratio on 10 of the 14 datasets. The use of FIFO queues enables S3-FIFO to achieve good scalability with 6\u00d7 higher throughput compared to optimized LRU at 16 threads. </p> <p>Our insight is that most objects in skewed cache workloads will only be accessed once in a short window, so it is critical to evict them early. And the key of S3-FIFO is a small FIFO queue that filters out most objects from entering the main cache. We show that filtering with a small static FIFO queue has a guaranteed eviction time and higher eviction precision compared to state-of-the-art adaptive algorithms.</p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#background","title":"Background","text":"<p>Software caches, such as Memcached, database buffer pool, and page cache, are widely deployed today to speed up data access.  A cache should be  1. efficient / effective: it should provide a low miss ratio allowing most requests to be fulfilled by the fast cache;  2. performant: serving data from the cache should perform minimal operations; and  3. scalable: the number of cache hits it can serve per second grows with the number of CPU cores.  The soul of a cache is the eviction algorithm, which dictates a cache's efficiency, throughput, and scalability. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#lru-and-fifo-based-eviction","title":"LRU and FIFO based eviction","text":"<p>While FIFO and LRU are the classics of cache eviction algorithm, many eviction algorithms have been desined in the past few decades to pursue better efficiency, e.g., ARC,  2Q,  LIRS,  TinyLFU. Because conventional wisdom suggests that LRU provides lower miss ratio than FIFO (although we recently found it to be false), these advanced algorithms are often LRU-based, using different techniques and metrics on top of one or more LRU queues.  However, LRU-based algorithms suffer from two problems: (1) it requires two pointers per object, a large storage overhead for workloads consisting of small objects; and (2) it is not scalable because each cache hit requires promoting the requested object to the head of the queue guarded by locking. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#the-importance-of-simplicity-and-scalability","title":"The importance of simplicity and scalability","text":"<p>Modern CPUs have a large number of cores. For example, AMD EPYC 9654P has 192 cores/threads.  A cache's scalability measures how its throughput increases with the number of CPU cores.  Ideally, a cache's throughput would scale linearly with the number of CPU cores. However, read operations necessitate metadata updates under locking in LRU-based algorithms. Therefore, they cannot fully harness the computation power of modern CPUs.</p> <p>\"Predicting which pages will be accessed in the near future is a tricky task, and the kernel has evolved a number of mechanisms designed to improve its chances of guessing right. But the kernel not only often gets it wrong, it also can expend a lot of CPU time to make the incorrect choice\".   ---- Kernel developers</p> <p>A cache eviction algorithm's complexity also plays a critical role in its adoption.  While the complexity is often correlated with throughput, a simple design can also bring fewer bugs and reduced maintenance overhead. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#fifo-is-the-future","title":"FIFO is the future","text":"<p>While the eviction algorithm so far have been centered around LRU, we believe modern eviction algorithms should be designed with FIFO queues.  FIFO can be implemented using a ring buffer without per-object pointer metadata, and it does not promote an object upon each cache hit, thus removing the scalability bottleneck. Moreover, FIFO evicts objects in the same order as the insertion, which is a flash-friendly access pattern and minimizes flash writes and wearout.  However, FIFO falls behind LRU and state-of-the-art eviction algorithms in efficiency. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#observation-more-one-hit-wonders-than-you-would-have-expected","title":"Observation: more one-hit wonders than you would have expected","text":"<p>The term ``one-hit-wonder ratio'' measures the fraction of objects that are requested only once in a sequence. It is commonly used in content delivery networks (CDNs) due to large one-hit-wonder ratios. Although one-hit-wonder ratio varies between different types of cache workloads, we find that shorter request sequences with fewer objects often have higher one-hit-wonder ratios. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#a-toy-example","title":"A toy example","text":"<p> An illustration of one-hit-wonder ratio (fraction of objects accessed once) increases with shorter request sequence.  <p></p> <p>The figure above illustrates this observation using a toy example.  The request sequence comprises seventeen requests for five objects, out of which one object (E) is accessed once. Thus, the one-hit-wonder ratio for the sequence is 20%.</p> <p>{% katexmm %} Considering a shorter sequence from the \\(1^{st}\\) to the \\(7^{th}\\) request, two (C, D) of the four unique objects are requested only once, which leads to a one-hit-wonder of 50%.  Similarly, the one-hit-wonder ratio of a shorter sequence from the \\(1^{st}\\) to \\(4^{th}\\) request is 67%. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#examples-from-production-traces","title":"Examples from production traces","text":"<p>Does this hold on production cache workloads? </p> <p> An illustration of one-hit-wonder ratio on production traces. The full trace has 20% to 60% objects accessed once, however, shorter sequences have much higher one-hit-wonder ratio.  <p></p> <p>The figure above show a block cache trace (MSR hm_0) and a key-value trace from Twitter (cluster 52). The X-axis shows the fraction of objects in the trace (in linear and log scales).  Compared to the one-hit-wonder ratio of the full trace at 13% (Twitter) and 38% (MSR), a random sub-sequence containing 10% objects has a one-hit-wonder ratio of 26% on the Twitter trace and 75% on the MSR trace. The increase is more significant when the sequence length is further reduced. </p> <p> A box plot showing one-hit-wonder ratio distribution on 6594 production traces.  <p></p> <p>We further analyzed a large cache trace collection of 6594 traces (more details can be found in the result section), and we plot the one-hit-wonder ratio distribution in box plots.  Compared to the full traces with a median one-hit-wonder ratio of 26%, sequences containing 50% of the objects in the trace show a median one-hit-wonder ratio of 38%. Moreover, sequences with 10% and 1% of the objects exhibit one-hit-wonder ratios of 72% and 78%, respectively.  </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#implication-of-a-large-one-hit-wonder-ratio","title":"Implication of a large one-hit-wonder ratio","text":"<p>The traces we used in the analysis are mostly week-long with a few month-long.  Because the cache size is often much smaller than the trace footprint (the number of objects in the trace), evictions start after encountering a short sequence of requests. Our observation suggests that if the cache size is at 10% of the trace footprint, approximately 72% of the objects would not be reused before eviction. </p> <p> A huge portion of objects in the cache are not accessed before eviction when using LRU, similar observations can be found when using other algorithms.  <p></p> <p>We further corroborate the observation with cache simulations. The figure above shows the distribution of object frequency at eviction.  Our trace analysis shows that the Twitter trace has a 26% one-hit-wonder ratio for sequences of 10% trace length.  The simulation shows a similar result: 26% of the objects evicted by LRU have are not requested after insertion at the cache size of 10% of the trace footprint.  Similarly, the MSR trace exhibits a higher one-hit-wonder ratio of 75% for sequences of 10% trace length, and the simulations shows that 82% of the objects evicted by LRU have no reuse. </p> <p>It is evident that the cache should filter out these one-hit wonders because they occupy space without providing benefits.</p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#s3-fifo-an-eviction-algorithm-with-only-fifo-queues","title":"S3-FIFO: an eviction algorithm with only FIFO queues","text":"<p>Motivated by the observation in the previous section,  we have designed a new cache eviction algorithm called S3-FIFO: S**imple, **S**calable caching with **three **S**tatic FIFO queues. </p> <p> An illustration of S3-FIFO.  <p></p> <p>S3-FIFO uses three FIFO queues: a small FIFO queue (S), a main FIFO queue (M), and a ghost FIFO queue (G).  We choose S to use 10% of the cache space based on experiments with 10 traces and find that 10% generalizes well.  M then uses 90% of the cache space. The ghost queue G stores the same number of ghost entries (no data) as M. </p> <p>Cache read:  S3-FIFO uses two bits per object to track object access status similar to a capped counter with frequency up to 3<sup>1</sup>.  Cache hits in S3-FIFO increment the counter by one atomically. Note that most requests for popular objects require no update. </p> <p>Cache write: New objects are inserted into S if not in G. Otherwise, it is inserted into M.  When S is full, the object at the tail is either moved to M if it is accessed more than once or G if not.  And its access bits are cleared during the move. When G is full, it evicts objects in FIFO order.  M uses an algorithm similar to FIFO-Reinsertion but tracks access information using two bits.  Objects that have been accessed at least once are reinserted with one bit set to 0 (similar to decreasing frequency by 1). </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#implementation","title":"Implementation","text":"<p>Although S3-FIFO has three FIFO queues, it can also be implemented with one or two FIFO queue(s).  Because objects evicted from S may enter M, they can be implemented using one queue with a pointer pointed at the 10% mark.  However, combining S and M reduces scalability because removing objects from the middle of the queue requires locking. </p> <p>The ghost FIFO queue G can be implemented as part of the indexing structure.  For example, we can store the fingerprint and eviction time of ghost entries in a bucket-based hash table.  The fingerprint stores a hash of the object using 4 bytes, and the eviction time is a timestamp measured in the number of objects inserted into G.  We can find out whether an object is still in the queue by calculating the difference between current time and insertion time since it is a FIFO queue.  The ghost entries stay in the hash table until they are no longer in the ghost queue. When an entry is evicted from the ghost queue, it is not immediately removed from the hash table. Instead, the hash table entry is removed during hash collision --- when the slot is needed to store other entries.  </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#how-does-s3-fifo-compare-to-other-algorithms","title":"How does S3-FIFO compare to other algorithms","text":""},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#dataset-we-used-in-this-blog","title":"Dataset we used in this blog","text":"<p>We evaluated S3-FIFO using a large collection of 6594 production traces from 14 datasets, including 11 open-source and 3 proprietary datasets. These traces span from 2007 to 2023 and cover key-value, block, and object CDN caches. In total, the datasets contain 856 billion requests to 61 billion objects, 21,088 TB traffic for total 3,573 TB of data.  More details of the datasets can be found in the table. </p> Dataset collections Approx time Cache type time span  (days) Traces Request  (million) Request  (TB) Object  (million) Object  (TB) One-hit-wonder ratio (full trace) One-hit-wonder ratio (10%) One-hit-wonder ratio (1%) MSR 2007 Block 30 13 410 10 74 3 0.56 0.74 0.86 FIU 2008-11 Block 9-28 9 514 1.7 20 0.057 0.28 0.91 0.91 Cloudphysics 2015 Block 7 106 2,114 82 492 22 0.40 0.71 0.80 CDN 1 2018 Object 7 219 3,728 3640 298 258 0.42 0.58 0.70 Tencent Photo 2018 Object 8 2 5,650 141 1,038 24 0.55 0.66 0.74 WikiMedia CDN 2019 Object 7 3 2,863 200 56 13 0.46 0.60 0.80 Systor 2017 Block 26 6 3,694 88 421 15 0.37 0.80 0.94 Tencent CBS 2020 Block 8 4030 33,690 1091 551 66 0.25 0.73 0.77 Alibaba 2020 Block 30 652 19,676 664 1702 117 0.36 0.68 0.81 Twitter 2020 KV 7 54 195,441 106 10,650 6 0.19 0.32 0.42 Social Network 1 2020 KV 7 219 549,784 392 42,898 9 0.17 0.28 0.37 CDN 2 2021 Object 7 1273 37,460 4,925 2,652 1,581 0.49 0.58 0.64 Meta KV 2022 KV 1 5 1,644 958 82 76 0.51 0.53 0.61 Meta CDN 2023 Object 7 3 231 8,800 76 1,563 0.61 0.76 0.81"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#experiemnt-setup","title":"Experiemnt setup","text":"<p>We implemented S3-FIFO and state-of-the-art algorithms in libCacheSim, and used an in house distributed computation platform for running the large-scale evaluation. Unless otherwise mentioned, we ignore object size because most production systems use slab storage for memory management, for which evictions are performed within the same slab class (objects of similar sizes).  Because the large number of traces have a very wide range of miss ratios, we choose to present the miss ratio reduction compared to FIFO.  We have also implemented a prototype in Cachelib, the details can be found in our paper.  The simulation processed the datasets in close to 100 passes using different algorithms, cache sizes, and parameters.  We estimated that over 80,000 billion requests were processed using a million CPU core \u2022 hours.</p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#efficiency-results","title":"Efficiency results","text":"<p>The primary criticism of the FIFO-based eviction algorithms is their efficiency, the most important metric for a cache.  We compare S3-FIFO with 12 state-of-the-art eviction algorithms designed in the past few decades. We use a cache size of 10% of the trace footprint (number of objects in the trace). Other cache sizes show similar results. </p> <p> Miss ratio reduction distribution of different algorithms, the cache size is 10% of objects in the trace (more figures in the paper).  <p></p> <p>The figure above shows the (request) miss ratio reduction (compared to FIFO) of different algorithms across traces.  S3-FIFO has the largest reductions across almost all percentiles than other algorithms. For example, S3-FIFO reduces miss ratios by more than 32% on 10% of the traces (P90) with a mean of 14%. </p> <p>TinyLFU is the closest competitor. TinyLFU uses a 1% LRU window to filter out unpopular objects and stores most objects in a SLRU cache. TinyLFU's good performance corroborates our observation that quick demotion is critical for efficiency.  However, TinyLFU does not work well for all traces, with miss ratios being lower than FIFO on almost 20% of the traces (the P10 point is below -0.05 and not shown in the figure).  This phenomenon is more pronounced when the cache size is small, where TinyLFU is worse than FIFO on close to 50% of the traces (not shown). </p> <p>There are two reasons why TinyLFU falls short. First, the 1% window LRU is too small, evicting objects too fast.  Therefore, increasing the window size to 10% of the cache size (TinyLFU-0.1) significantly improves the efficiency at the tail (bottom of the figure). However, increasing the window size reduces its improvement on the best-performing traces.  Second, when the cache is full, TinyLFU compares the least recently used entry from the window LRU and main SLRU, then evicts the less frequently used one. This allows TinyLFU to be more adaptive to different workloads. However, if the tail object in the SLRU happens to have a very high frequency, it may lead to the eviction of an excessive number of new and potentially useful objects. </p> <p>LIRS uses LRU stack (reuse) distance as the metric to choose eviction candidates. Because one-hit wonders do not have reuse distance, LIRS utilizes a 1% queue to hold them. This small queue performs quick demotion and is the secret source of LIRS's high efficiency. Similar to TinyLFU, the queue is too small, and it falls short on some cache workloads.  However, compared to TinyLFU, fewer traces show higher-than-FIFO miss ratios because the inter-recency metric in LIRS is more robust than the frequency in TinyLFU. In particular, TinyLFU cannot distinguish between many objects with the same low frequency (e.g., 2), but these objects will have different inter-recency values. The downside is that LIRS requires a more complex implementation than TinyLFU. </p> <p>2Q has the most similar design to S3-FIFO. It uses 25% cache space for a FIFO queue, the rest for an LRU queue, and also has a ghost queue. Besides the difference in queue size and type, objects evicted from the small queue are \\emph{not} inserted into the LRU queue.  Having a large probationary queue and not moving accessed objects into the LRU queue are the primary reasons why 2Q is not as good as S3-FIFO.  Moreover, we observe that the LRU queue does not provide observable benefits compared to the FIFO queue (with reinsertion) in S3-FIFO. </p> <p>SLRU uses four equal-sized LRU queues. Objects are first inserted into the lowest-level LRU queue and promoted to higher-level queues upon cache hits. An inserted object is evicted if not reused in the lowest LRU queue, which performs quick demotion and allows SLRU to show good efficiency. However, unlike other schemes, SLRU does not use a ghost queue, making it not scan-tolerant because popular objects mixed in the scan cannot be distinguished. Therefore, we observe that SLRU performs poorly on many block cache workloads (not shown). </p> <p>ARC uses four LRU queues: two for data and two for ghost entries. The two data queues are used to separate recent and frequent objects. Cache hits on objects in the recency queue promote the objects to the frequency queue. Objects evicted from the two data queues enter the corresponding ghost queue. The sizes of queues are adaptively adjusted based on hits on the ghost queues. When the recency queue is small, newly inserted objects are quickly evicted, enabling ARC's high efficiency. However, ARC is less efficient than S3-FIFO because the adaptive algorithm is not sufficient. We discuss more in \\S\\ref{sec:discussion:adaptive}. </p> <p>Recent algorithms, including CACHEUS, LeCaR, LHD, and FIFO-Merge are also evaluated. However, we find these algorithms are often less competitive than the traditional ones. In particular, FIFO-merge was designed for log-structured storage and key-value cache workloads without scan resistance. Therefore, similar to SLRU, it performs better on web cache workloads but much worse on block cache workloads. </p> <p>Common algorithms, such as B-LRU (Bloom Filter LRU), CLOCK, and LRU. CLOCK and LRU do not allow quick demotion, so their miss ratio reductions are small.  B-LRU is the other extreme. It rejects all one-hit wonders at the cost of the second request for all objects being cache misses. Because of these misses, B-LRU is worse than LRU in most cases. </p> <p>Adversarial workloads for S3-FIFO:  We studied the limited number of traces on which S3-FIFO performed poorly and identified one pattern.  Most objects in these traces are accessed only twice, and the second request falls out of the small FIFO queue S, which causes the second request to these objects to be cache misses. These workloads are adversarial for most algorithms that partition the cache space, e.g., TinyLFU, LIRS, 2Q, and CACHEUS.  Because the partition for newly inserted objects is smaller than the cache size, it is possible that the second request is a cache hit in LRU and FIFO, but not in these advanced algorithms. </p> <p> Mean miss ratio reduction of different state-of-the-art algorithms (more results in the paper). S3-FIFO is the best on 10 of the 14 datasets.  <p></p> <p>Not only being efficient, the efficient of S3-FIFO is also robust.  The figure above shows the mean miss ratio reduction on each dataset using selected algorithms. S3-FIFO often outperforms all other algorithms by a large margin. Moreover, it is the best algorithm on 10 out of the 14 datasets, and among the top three most efficient algorithms on 13 datasets.  As a comparison, TinyLFU and LIRS are among the top algorithms on some datasets, but on other datasets, they are among the worst algorithms. </p> <p>While (request) miss ratio is important for most, if not all, cache deployments, CDNs also widely use byte miss ratio to measure bandwidth reduction. Compared to other algorithms, S3-FIFO presents larger byte miss ratio reductions similar to the figure shown. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#throughput-results","title":"Throughput results","text":"<p>It is easy to see that as a FIFO-base algorithm, S3-FIFO is more scalable than LRU-based algorithms.  Our implementation in Cachelib achieves 6x higher throughput than the optimized LRU in Cachelib at 16 threads.  More details can be found in the paper. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#why-dont-adaptive-algorithms-work-well","title":"Why don't adaptive algorithms work well?","text":"<p>There are several adaptive algorithms, e.g., ARC and TinyLFU, that also perform quick demotion and should work as least as good as S3-FIFO in theory.  Where do they fall short? We take a closer look at demotion speed and precision to get a deeper understanding. The normalized quick demotion speed measures how long objects stay in S before they are evicted or moved to M.  {% katexmm %} We use the LRU eviction age as a baseline and calculate the speed as \\(\\frac{\\text{LRU eviction age}}{\\text{time in } \\mathcal{S}}\\). Here we use logical time measured in request count.  The quick demotion precision measures how many objects evicted from S are not reused soon. Using an idea similar to previous work~\\cite{song_learning_2020}, if the number of requests till an object's next reuse is larger than \\(\\frac{\\text{cache size}}{\\text{miss ratio}}\\), then we say the quick demotion results in a correct early eviction.</p> <p>An algorithm with both faster and more precise quick demotion exhibits a lower miss ratio. </p> <p> The normalized mean quick demotion speed and precision of different algorithms. TinyLFU and S3-FIFO use different small queue sizes (1%, 2%, 5%, 10%, 20%, 30%, and 40% of cache size) and have multiple points with lighter colors representing larger sizes. The marker of 10% small queue size is highlighted with a larger size. The left figure shows the Twitter workload, and the right figure shows the MSR workload.  <p></p> <p>Miss ratios on the Twitter and MSR traces when using different S sizes.  On the Twitter trace, ARC has a miss ratio 0.0483, LRU has miss ratio 0.0488. On the MSR trace, ARC has a miss ratio 0.2891, LRU miss ratio 0.3188. </p> Trace S size 0.01 0.02 0.05 0.10 0.20 0.30 0.40 Twitter TinyLFU 0.0437 0.0437 0.0586 0.0530 0.0441 0.0445 0.0451 Twitter S3-FIFO 0.0423 0.0422 0.0422 0.0424 0.0432 0.0442 0.0455 MSR TinyLFU 0.2895 0.2904 0.2893 0.2900 0.2936 0.2949 0.2990 MSR S3-FIFO 0.2889 0.2887 0.2884 0.2891 0.2896 0.2936 0.2989 <p>The figure and table above show that ARC, TinyLFU, and S3-FIFO can quickly demote new objects and have lower miss ratios compared to LRU.</p> <p>ARC uses an adaptive algorithm to decide the size of S (recency queue). We find that the algorithm can identify the correct direction to adjust the size, but the size it finds is often too large or too small. For example, ARC chooses a very small S on the Twitter trace, causing most new objects to be evicted too quickly with low precision.  This happens because of two trace properties. First, objects in the Twitter trace often have many requests; Second, new objects are constantly generated.  Therefore, objects evicted from M are requested very soon, causing S to shrink to a very small size (around 0.01% of cache size).  Meanwhile, constantly generated new (and popular) objects in S face more competition and often have to suffer a miss before being inserted in M, which causes low precision and a high miss ratio (Table~\\ref{table:missratio}).  On the MSR trace, ARC has a reasonable speed with relatively high precision, which correlates with its low miss ratio. </p> <p>TinyLFU and S3-FIFO have a predictable quick demotion speed --- reducing the size of S always increases the demotion speed.  When Using the same S size, TinyLFU demotes slightly faster than S3-FIFO because it uses LRU, which keeps some old but recently-accessed objects, squeezing the available space for newly-inserted objects. </p> <p>Besides, S3-FIFO often shows higher precision than TinyLFU at a similar quick demotion speed, which explains why S3-FIFO has a lower miss ratio.  TinyLFU compares the eviction candidates from S and M, then evicts the less-frequently-used candidate. When the eviction candidate from M has a high frequency, it causes many worth-to-keep objects from S to be evicted.  This causes not only a low precision but also unpredictable precision and miss ratio cliffs.  For example, the precision shows a large dip at 5% and 10% S size, corresponding to a sudden increase in the miss ratio (in the table). </p> <p>Although S3-FIFO does not use advanced techniques, it achieves a robust and predictable quick demotion speed and precision.  As S size increases, the speed decreases monotonically (moving towards the left in the figure), and the precision also increases until it reaches a peak.  When S is very small, popular objects do not have enough time to accumulate a hit before being evicted, so the precision is low. Increasing S size leads to higher precision. When S is very large, many unpopular objects are requested in S and moved to M, leading to reduced precision as well.  The miss ratio presented in the table shows that at similar quick demotion speed, higher precision always leads to lower miss ratios.  </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#can-we-tune-the-adaptive-algorithms-to-work-better","title":"Can we tune the adaptive algorithms to work better?","text":"<p>We have also experimented with using an adaptive algorithm to change the size of FIFO queues in S3-FIFO. The results show that using an adaptive algorithm can improve the tail performance, but degrade overall performance.  We find that tuning the adaptive algorithm is very challenging. </p> <p>In fact, adaptive algorithms all have many parameters.  For example, queue resizing requires several parameters, e.g., the frequency of resizing, the amount of space moved each time, the lower bound of queue sizes, and the threshold for trigger resizing. </p> <p>Besides the many hard-to-tune parameters, adaptive algorithms adapt based on observation of the past. However, the past may not predict the future. We find that small perturbations in the workload often cause the adaptive algorithm to overreact.  It is unclear how to balance between under-reaction and overreaction without introducing more parameters.  Moreover, some adaptive algorithms implicitly assume that the miss ratio curve is convex because following the gradient direction leads to the global optimum. However, the miss ratio curves of scan-heavy workloads are often not convex. </p> <p>Although we have shown that S3-FIFO is not sensitive to S size, and the queue size is easier to choose than tuning an adaptive algorithm. We believe adaptations are still important, but how to adapt remains to be explored.  For systems that need to find the best parameter, downsized simulations using spatial sampling can be used.</p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#conclusion","title":"Conclusion","text":"<p>We demonstrate that a cache often experiences a higher one-hit-wonder ratio than common full trace analysis.  Our study on 6594 traces reveals that quickly removing one-hit wonders (quick demotion) is the secret weapon of many advanced algorithms.  Motivated by this, we design S3-FIFO, a **S**imple and **S**calable cache eviction algorithm composed of only **S**tatic FIFO queues.  Our evaluation shows that S3-FIFO achieves better and more robust efficiency than state-of-the-art algorithms. Meanwhile, it is more scalable than LRU-based algorithms. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#acknowledgement","title":"Acknowledgement","text":"<p>There are many people I would like to thank, including but not limited to my co-authors, Carnegie Mellon University Parallel Data Lab (and our sponsors), and Cloudlab.  I would also like to give a big shoutout to the people and organizations that open-sourced and shared the traces. </p>"},{"location":"blog/2023/08/01/fifo-queues-are-all-you-need-for-cache-eviction/#dataset-information","title":"Dataset information","text":"<ul> <li>Twitter</li> <li>Tencent Block</li> <li>Tencent Photo</li> <li>Wikimedia CDN</li> <li>Alibaba Block</li> <li>MSR</li> <li>FIU</li> <li>CloudPhysics</li> <li>Meta</li> </ul> <p>This work is published at SOSP'23, more details can be found here, and the slides can be found here. </p> <ol> <li> <p>We have also experienced with 1-bit counter, which shows slightly worse efficiency; however, we choose to use 2-bit because it is more robust and filters out more objects.\u00a0\u21a9</p> </li> </ol>"}]}